{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473f9d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.1264337411204588\n",
      "Epoch [2/10], Loss: 1.0996047176339843\n",
      "Epoch [3/10], Loss: 1.0719989789240296\n",
      "Epoch [4/10], Loss: 1.0470329004141483\n",
      "Epoch [5/10], Loss: 1.024531399332272\n",
      "Epoch [6/10], Loss: 1.003704953625977\n",
      "Epoch [7/10], Loss: 0.9846524847602245\n",
      "Epoch [8/10], Loss: 0.9671675525073798\n",
      "Epoch [9/10], Loss: 0.9506875607097306\n",
      "Epoch [10/10], Loss: 0.9353960195587132\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Simple tokenizer function\n",
    "def simple_tokenizer(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "    return text.split()\n",
    "\n",
    "# Vocabulary class to handle word to index mapping\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Dataset class to load and preprocess text data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset_split):\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "        self.vocab = Vocabulary()\n",
    "        self.max_len = 0  # Initialize max_len\n",
    "        \n",
    "        # Extract pairs and labels\n",
    "        for i in range(len(dataset_split)):\n",
    "            sentence1 = dataset_split[i]['sentence1']\n",
    "            sentence2 = dataset_split[i]['sentence2']\n",
    "            label = dataset_split[i]['label']\n",
    "            self.pairs.append((sentence1, sentence2))\n",
    "            self.labels.append(label)\n",
    "            \n",
    "            # Tokenize and add to vocab\n",
    "            tokens1 = simple_tokenizer(sentence1)\n",
    "            tokens2 = simple_tokenizer(sentence2)\n",
    "            for token in tokens1 + tokens2:\n",
    "                self.vocab.add_word(token)\n",
    "            \n",
    "            # Update max_len for consistent padding\n",
    "            self.max_len = max(self.max_len, len(tokens1), len(tokens2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence1, sentence2 = self.pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        indices1 = [self.vocab.word2idx[token] for token in simple_tokenizer(sentence1) if token in self.vocab.word2idx]\n",
    "        indices2 = [self.vocab.word2idx[token] for token in simple_tokenizer(sentence2) if token in self.vocab.word2idx]\n",
    "        \n",
    "        padded1 = indices1 + [0] * (self.max_len - len(indices1))\n",
    "        padded2 = indices2 + [0] * (self.max_len - len(indices2))\n",
    "        \n",
    "        return torch.tensor(padded1), torch.tensor(padded2), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Model for training word embeddings\n",
    "class WordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        mask = (inputs != 0).unsqueeze(-1).type(torch.float32)  # Create a mask for non-padding tokens\n",
    "        embeds_masked = embeds * mask  # Apply the mask to zero out padding embeddings\n",
    "        # Sum embeddings along the time dimension (dim=1) and divide by the number of non-zero elements in each sequence\n",
    "        sentence_embeddings = embeds_masked.sum(dim=1) / mask.sum(dim=1)\n",
    "        return sentence_embeddings\n",
    "\n",
    "\n",
    "# Contrastive loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "# Load dataset splits\n",
    "dataset = load_dataset(\"PiC/phrase_similarity\")\n",
    "train_dataset = TextDataset(dataset['train'])\n",
    "dev_dataset = TextDataset(dataset['validation'])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Model and training setup\n",
    "embedding_dim = 100\n",
    "model = WordEmbeddingModel(len(train_dataset.vocab), embedding_dim, pad_idx=0)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = ContrastiveLoss(margin=2.0)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for sentence1, sentence2, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output1 = model(sentence1)\n",
    "        output2 = model(sentence2)\n",
    "        loss = criterion(output1, output2, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/10], Loss: {total_loss / len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d746845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shapes: [torch.Size([100]), torch.Size([100])]\n",
      "Similarity: 0.04329507797956467\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def get_sentence_embeddings(sentences, model, vocab, max_len=30):\n",
    "    sentence_indices = []\n",
    "    for sentence in sentences:\n",
    "        tokens = simple_tokenizer(sentence)\n",
    "        indices = [vocab.word2idx[token] for token in tokens if token in vocab.word2idx]\n",
    "        padded_indices = indices + [0] * (max_len - len(indices))\n",
    "        sentence_indices.append(padded_indices)\n",
    "    \n",
    "    sentence_tensor = torch.tensor(sentence_indices)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(sentence_tensor)\n",
    "    return embeddings\n",
    "\n",
    "def compare_sentence_similarity(embedding1, embedding2):\n",
    "    similarity = cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0))\n",
    "    return similarity.item()\n",
    "\n",
    "\n",
    "\n",
    "sentences = [\"Disciplined student\",\"The fruit is very tasty\"]\n",
    "#embedding dimensions\n",
    "embeddings = get_sentence_embeddings(sentences, model, train_dataset.vocab)\n",
    "print(\"Embedding shapes:\", [e.shape for e in embeddings])\n",
    "\n",
    "# If the embedding shapes are correct, compute similarity\n",
    "if all(e.dim() == 1 for e in embeddings):\n",
    "    similarity = compare_sentence_similarity(embeddings[0], embeddings[1])\n",
    "    print(f\"Similarity: {similarity}\")\n",
    "else:\n",
    "    print(\"Error: Embeddings are not 1D vectors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16d32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349b1332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shapes: [torch.Size([100]), torch.Size([100])]\n",
      "Similarity: 0.9591876864433289\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def get_sentence_embeddings(sentences, model, vocab, max_len=30):\n",
    "    sentence_indices = []\n",
    "    for sentence in sentences:\n",
    "        tokens = simple_tokenizer(sentence)\n",
    "        indices = [vocab.word2idx[token] for token in tokens if token in vocab.word2idx]\n",
    "        padded_indices = indices + [0] * (max_len - len(indices))\n",
    "        sentence_indices.append(padded_indices)\n",
    "    \n",
    "    sentence_tensor = torch.tensor(sentence_indices)\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        embeddings = model(sentence_tensor)\n",
    "    return embeddings\n",
    "\n",
    "def compare_sentence_similarity(embedding1, embedding2):\n",
    "    similarity = cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0))\n",
    "    return similarity.item()\n",
    "\n",
    "\n",
    "\n",
    "sentences = [\"A major criticism of litigation funding is that its cost is disproportionate to the risk accepted by litigation finance companies.\", \"A Harsh outspoken discussion of litigation funding is that its cost is disproportionate to the risk accepted by litigation finance companies.\"]\n",
    "\n",
    "embeddings = get_sentence_embeddings(sentences, model, train_dataset.vocab)\n",
    "print(\"Embedding shapes:\", [e.shape for e in embeddings])\n",
    "\n",
    "# If the embedding shapes are correct, compute similarity\n",
    "if all(e.dim() == 1 for e in embeddings):\n",
    "    similarity = compare_sentence_similarity(embeddings[0], embeddings[1])\n",
    "    print(f\"Similarity: {similarity}\")\n",
    "else:\n",
    "    print(\"Error: Embeddings are not 1D vectors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4afac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab3c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae845e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959f2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1a521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca316401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8d0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505fed1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b45562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5136877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928afbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e0ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e310a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564f3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb9147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da261389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42154d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fe450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bdd91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
