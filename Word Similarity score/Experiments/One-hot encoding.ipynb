{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3643367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample vocabulary\n",
    "vocabulary = ['apple', 'banana', 'orange', 'fruit', 'sweet']\n",
    "\n",
    "# OneHotEncoder from sklearn to fit on the vocabulary\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoded = encoder.fit_transform(np.array(vocabulary).reshape(-1, 1))\n",
    "\n",
    "# Function to convert a word to one-hot encoded vector\n",
    "def word_to_one_hot(word):\n",
    "    index = vocabulary.index(word)\n",
    "    return one_hot_encoded[index]\n",
    "\n",
    "# Example\n",
    "word_to_one_hot('apple')  # This will return the one-hot encoded vector for 'apple'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10fd33d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sevan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.56351421e-02, -1.90203730e-02, -4.11062239e-04,  6.93839323e-03,\n",
       "       -1.87794445e-03,  1.67635437e-02,  1.80215668e-02,  1.30730132e-02,\n",
       "       -1.42324204e-03,  1.54208085e-02, -1.70686692e-02,  6.41421322e-03,\n",
       "       -9.27599426e-03, -1.01779103e-02,  7.17923651e-03,  1.07406788e-02,\n",
       "        1.55390287e-02, -1.15330126e-02,  1.48667218e-02,  1.32509926e-02,\n",
       "       -7.41960062e-03, -1.74912829e-02,  1.08749345e-02,  1.30195115e-02,\n",
       "       -1.57510047e-03, -1.34197120e-02, -1.41718509e-02, -4.99412045e-03,\n",
       "        1.02865072e-02, -7.33047491e-03, -1.87401194e-02,  7.65347946e-03,\n",
       "        9.76895820e-03, -1.28571270e-02,  2.41711619e-03, -4.14975407e-03,\n",
       "        4.88066689e-05, -1.97670180e-02,  5.38400887e-03, -9.50021297e-03,\n",
       "        2.17529293e-03, -3.15244915e-03,  4.39334614e-03, -1.57631524e-02,\n",
       "       -5.43436781e-03,  5.32639725e-03,  1.06933638e-02, -4.78302967e-03,\n",
       "       -1.90201886e-02,  9.01175756e-03], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"Apple and banana are fruits\", \"Fruits like apple and orange are sweet\"]\n",
    "\n",
    "# Tokenizing words\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Training Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get vector for a word\n",
    "def get_word_vector(word):\n",
    "    return model.wv[word]\n",
    "\n",
    "# Example\n",
    "get_word_vector('apple')  # This will return the 50-dimensional vector for 'apple'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dad3f6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044917308"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(word1, word2):\n",
    "    vector1 = get_word_vector(word1).reshape(1, -1)\n",
    "    vector2 = get_word_vector(word2).reshape(1, -1)\n",
    "    return cosine_similarity(vector1, vector2)[0][0]\n",
    "\n",
    "# Example\n",
    "calculate_similarity('apple', 'banana')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54092414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17888929\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\sevan\\Desktop\\IIIT-H\\SimLex-999\\SimLex-999\\SimLex-999.txt\", delimiter='\\t')\n",
    "\n",
    "# Extract words and their pairs\n",
    "word_pairs = data[['word1', 'word2']].values\n",
    "\n",
    "# Flatten the list of pairs and tokenize\n",
    "all_words = [word for pair in word_pairs for word in pair]\n",
    "\n",
    "# You might want to process the list to handle duplicates, etc.\n",
    "sentences = [word_tokenize(\" \".join(set(all_words)))]\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences=sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get vector for a word\n",
    "def get_word_vector(word):\n",
    "    return model.wv[word]\n",
    "\n",
    "# Function to calculate similarity between two word vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(word1, word2):\n",
    "    vector1 = get_word_vector(word1).reshape(1, -1)\n",
    "    vector2 = get_word_vector(word2).reshape(1, -1)\n",
    "    return cosine_similarity(vector1, vector2)[0][0]\n",
    "\n",
    "# Example usage\n",
    "print(calculate_similarity('fast', 'rapid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a9a4513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41072547\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(calculate_similarity('fast', 'rapid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20c263de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.18896222\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(calculate_similarity('apple', 'juice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8dd57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527cab90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e5110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f6dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32caf9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5666f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1aefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ffb663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00949705  0.00957051 -0.00776096 -0.00263575 -0.00490457 -0.00497958\n",
      " -0.00801145 -0.00776483 -0.0045593  -0.00129684 -0.00509552  0.00613002\n",
      " -0.00952529 -0.00530443  0.00943686  0.00699585  0.00769379  0.00424049\n",
      "  0.00049711 -0.00599661  0.00603068  0.00263671  0.0077129   0.00638569\n",
      "  0.00793799  0.00866407 -0.00990478 -0.00674524  0.00133512  0.0064464\n",
      "  0.00737509  0.0055081   0.00766372 -0.00514982  0.00658349 -0.00411261\n",
      " -0.0090392   0.00914917  0.00133302 -0.00275518 -0.00246698 -0.00422915\n",
      "  0.00480578  0.0044081  -0.00264758 -0.00734272 -0.00357629 -0.00033629\n",
      "  0.00611082 -0.00282893 -0.00012132  0.00087327 -0.00709177  0.00205667\n",
      " -0.00143795  0.00280997  0.00485269 -0.00134754 -0.00278216  0.0077516\n",
      "  0.00504399  0.00671702  0.00451951  0.00867088  0.00747768 -0.0010623\n",
      "  0.00875303  0.00461498  0.00543825 -0.00138768 -0.00203692 -0.00442776\n",
      " -0.00850421  0.00303949  0.00888956  0.00891639 -0.00193567  0.00609334\n",
      "  0.00378064 -0.00431056  0.00202129 -0.00543464  0.00821465  0.00543129\n",
      "  0.00318581  0.00410031  0.00867398  0.00727639 -0.00083548 -0.00708057\n",
      "  0.00839455  0.00723027  0.00173359 -0.00134208 -0.0058782  -0.00453384\n",
      "  0.00864828 -0.0031424  -0.00634771  0.00988009]\n",
      "-0.033145506\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text data (you would replace this with your corpus)\n",
    "sample_text = \"Hello, my name is Sevanth Gajula. I'm currently in my final year of my undergraduation. I want to now work on LLM's. The last thing I want to do is sleep.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = [word_tokenize(doc.lower()) for doc in nltk.sent_tokenize(sample_text)]\n",
    "\n",
    "# Training a Word2Vec model\n",
    "model = Word2Vec(tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Getting word vectors\n",
    "vector = model.wv['currently']  # replace 'example' with any word\n",
    "\n",
    "print(vector)\n",
    "\n",
    "# Calculating similarity between two words\n",
    "similarity_score = model.wv.similarity('last', 'final')  # replace 'word1' and 'word2' with actual words\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "622d94b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Context Score: -0.013516951\n",
      "Different Context Score: -0.042464238\n",
      "Contrasting Contexts Score: 0.031036329\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text data\n",
    "sample_texts = [\n",
    "    \"In my final year of university, the last semester was the most challenging.\",\n",
    "    \"The final decision rests with the last committee meeting.\",\n",
    "    \"The last time I saw him, it was at the final match of the season.\"\n",
    "]\n",
    "\n",
    "# Tokenization\n",
    "tokens = [word_tokenize(doc.lower()) for doc in sample_texts]\n",
    "\n",
    "# Training a Word2Vec model\n",
    "model = Word2Vec(tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Calculating similarity scores\n",
    "similar_context_score = model.wv.similarity('last', 'final')\n",
    "different_context_score = model.wv.similarity('last', 'committee')\n",
    "contrasting_contexts_score = model.wv.similarity('last', 'time')\n",
    "\n",
    "print(\"Similar Context Score:\", similar_context_score)\n",
    "print(\"Different Context Score:\", different_context_score)\n",
    "print(\"Contrasting Contexts Score:\", contrasting_contexts_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
